{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "p_tenant_id = \"<TENANT_ID>\"\n",
    "p_azure_key_vault = \"<AZURE_KEY_VAULT>\"\n",
    "p_workspace_id=\"<WORKSPACE_ID>\"\n",
    "p_lakehouse_id=\"<LAKEHOUSE_ID>\"\n",
    "p_table_name=\"<LAKEHOUSE_TABLE_NAME>\"\n",
    "# Key Vault secret keys for Service Principal\n",
    "p_secret_client_id = \"<SECRET_CLIENT_ID>\"\n",
    "p_secret_client_secret = \"<SECRET_CLIENT_SECRET>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_account_name = \"onelake\"\n",
    "workspace_id=p_workspace_id\n",
    "lakehouse_id=p_lakehouse_id\n",
    "table_name=p_table_name\n",
    "endpoint=f\"{storage_account_name}.dfs.fabric.microsoft.com\"\n",
    "path = f\"abfss://{workspace_id}@{endpoint}/{lakehouse_id}/Tables/{table_name}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "\n",
    "kv_uri = f\"https://{p_azure_key_vault}.vault.azure.net/\"\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "secret_client = SecretClient(vault_url=kv_uri, credential=credential)\n",
    "\n",
    "client_id = secret_client.get_secret(p_secret_client_id).value\n",
    "client_secret = secret_client.get_secret(p_secret_client_secret).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"fabric_conn\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.hadoop:hadoop-azure:3.3.1,\"\n",
    "            \"org.apache.hadoop:hadoop-azure-datalake:3.3.1,\"\n",
    "            \"io.delta:delta-spark_2.12:3.0.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{endpoint}\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{endpoint}\",  \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{endpoint}\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{endpoint}\",client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{endpoint}\", f\"https://login.microsoftonline.com/{p_tenant_id}/oauth2/token\")\n",
    "\n",
    "print(f\"ABFS Path: {path}\")\n",
    "df = spark.read.format(\"delta\").load(path)\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
